# âš¡ FFT Benchmark Lab

A Python benchmarking toolkit (originally developed for the 01204496-65 DSP course at Kasetsart University) for evaluating custom FFT implementations against SciPy.  

**Key capabilities:**
- Hand-written FFT algorithms (recursive, iterative, radix-4, etc.)  
- Automated error analysis (MAE, MSE, pass/fail) and speed benchmarking  
- CLI controls: `--mode`, `--minimal`, `--save-csv`  
- Colorized terminal output and organized, timestamped CSV results  

## ğŸ“œ Table of Contents

- [âš¡ FFT Benchmark Lab](#-fft-benchmark-lab)
  - [ğŸ“œ Table of Contents](#-table-of-contents)
  - [ğŸ“‚ Project Structure](#-project-structure)
  - [ğŸ“¥ Getting Started](#-getting-started)
    - [Installation](#installation)
    - [Usage](#usage)
    - [Running Benchmark](#running-benchmark)
    - [Optional flags](#optional-flags)
    - [Custom Implementations](#custom-implementations)
    - [Listing Registered FFT Implementations](#listing-registered-fft-implementations)
  - [ğŸ“Š Example Output](#-example-output)
    - [Console Output](#console-output)
    - [CSV Output Format](#csv-output-format)
  - [ğŸ“„ License](#-license)
  - [ğŸ§‘â€ğŸ’» Contributors](#-contributors)


## ğŸ“‚ Project Structure

```
project/
â”œâ”€â”€ fft_core/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ selection.py       # Helper file for importing FFT implementations
â”‚   â””â”€â”€ ...                # FFT implementations
â”‚
â”œâ”€â”€ util/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ csv_utils.py       # CSV utilities for saving results
â”‚   â”œâ”€â”€ io_utils.py        # I/O utilities for colored and silent output
â”‚   â”œâ”€â”€ test_case.py       # Predefined test signals
â”‚   â””â”€â”€ test.py            # Benchmark and correctness wrapper
â”‚
â”œâ”€â”€ get_registered_fft.py  # CLI for listing registered FFT implementations
â”œâ”€â”€ main.py                # CLI entry point for benchmarking
â”œâ”€â”€ README.md              # Project overview (this file)
â”œâ”€â”€ uv.lock                # uv package lock file
â”œâ”€â”€ requirements.txt       # Python dependencies (if not using uv)
â””â”€â”€ pyproject.toml         # Project metadata
```

## ğŸ“¥ Getting Started


### Installation

**Clone this repository:**

```bash
git clone https://github.com/RJTPP/fft-benchmark-lab.git
cd fft-benchmark-lab
```

**Install Dependencies**

This project use [`uv`](https://github.com/astral-sh/uv) for package management and installation.

To install the project dependencies, run:

```bash
uv sync
```

However, you can also use `pip` if preferred:

```bash
pip install -r requirements.txt
```


### Usage

### Running Benchmark

To run the benchmark tests, execute the main script:

```bash
uv run main.py
```

Or if you prefer using `python`:

```bash
python main.py
```

### Optional flags

- `--mode [all|check|speed]` â€” Run only metrics tests, speed tests, or both (default: all)
- `--save-csv [DIR]` â€” Save results to a timestamped directory (e.g., `results_20250101_000000/`)
  - If no directory is provided, a default folder will be created
- `--minimal` â€” Reduce test output to minimal


### Custom Implementations

You can benchmark any function that takes a 1D `np.ndarray` and returns a `np.ndarray` (e.g., DFT, FFT, or other spectral transforms). 

By default, results are compared against `scipy.fft.fft`.

To add a new FFT implementation:

1. Create a new file in `fft_core/`, e.g. `fft_myalgo.py`.
2. Decorate your FFT function with `@register_fft` from `fft_core.selection`:
   ```python
    import numpy as np
    from fft_core.selection import register_fft

    @register_fft
    # Or use a custom name: @register_fft(name="myalgo")
    def fft(x: np.ndarray) -> np.ndarray:
        """Your FFT algorithm here"""
        return ...
   ```
    - Each function will be auto-registered by name.
    - If multiple functions share the same name, they will be renamed automatically (e.g., `fft`, `fft_1`, `fft_2`, â€¦).
    - The function must accept a 1D np.ndarray of complex values and return a transformed np.ndarray of the same shape and type.

3. The main script will automatically detect `fft_myalgo.fft` and include it in benchmarks.

> [!TIP]
> To organize your implementations, you can use subfolders in fft_core/ (with `__init__.py`), e.g. `fft_core/mygroup/fft_cool.py`

### Listing Registered FFT Implementations

To list all registered FFT implementations, run:

```bash
uv run get_registered_fft.py [-l | --list] [-v | --verbose]

# Or `python get_registered_fft.py`
```

## ğŸ“Š Example Output

### Console Output

```
ğŸ” Correctness Testing: simple_fft...
  âœ… Test case 1: PASS (MAE: 0, MSE: 0+0j)
  âœ… Test case 2: PASS (MAE: 0, MSE: 0+0j)
  ...

ğŸ• Speed Testing: simple_fft...
  âœ… Time (size: 1024): 17.92 Âµs (avg per bin: 0.018 Âµs)
  âœ… Time (size: 2048): 42.68 Âµs (avg per bin: 0.021 Âµs)
  ...

ğŸ—‚ï¸  Saving results...
  ğŸ’¾  Saved simple_fft metrics to ...
  ...
```


### CSV Output Format

Benchmark results are saved under the `/results/` folder in a timestamped subdirectory, e.g., `results/results_20250101_000000/`, with the following structure:

```
results/
 â”— results_YYYYMMDD_HHMMSS/  # Or the provided directory name
   â”£ metrics/
   â”ƒ â”£ FUNC_NAME.csv         # Individual metric results per function
   â”ƒ â”— ...
   â”£ speed/
   â”ƒ â”£ FUNC_NAME.csv         # Individual speed results per function
   â”ƒ â”— ...
   â”£ metrics.csv             # Combined metrics for all functions
   â”— speed.csv               # Combined speed for all functions
```

**metrics.csv** and metrics/FUNC_NAME.csv share the same format:

```csv
func,test_no,input_size,mae,mse,is_pass,is_error
```

- **func**: FFT function name  
- **test_no**: index of the test case  
- **input_size**: signal length  
- **mae**: mean absolute error  
- **mse**: mean squared error  
- **is_pass**: whether the result matched tolerance (`true`/`false`)  
- **is_error**: whether an exception occurred (`true`/`false`)  

**speed.csv** and speed/FUNC_NAME.csv share the same format:

```csv
func,test_no,input_size,time_used_us,time_per_bin_us,is_error
```

- **func**: FFT function name  
- **test_no**: index of the test case  
- **input_size**: signal length  
- **time_used_us**: total execution time in microseconds  
- **time_per_bin_us**: average time per FFT bin  
- **is_error**: whether an exception occurred during timing (`true`/`false`)  


## ğŸ“„ License

This project is licensed under the MIT License. See [LICENSE](LICENSE) for details.

## ğŸ§‘â€ğŸ’» Contributors

Rajata Thamcharoensatit ([@RJTPP](https://github.com/RJTPP))
